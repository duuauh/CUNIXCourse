import torch
from torchvision import datasets, transforms
import torch.nn.functional as F
import os
import torch.nn as nn
from torch.autograd import Variable
import torch.optim as optimizer

class Mynet(nn.Module):
	def __init__(self):
		super(Mynet, self).__init__()
		self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3)
		self.conv2 = nn.Conv2d(in_channels=10, out_channels=16, kernel_size=3)
		self.maxpool = nn.MaxPool2d(kernel_size=4)
		self.fc = nn.Linear(5184, 10)
		self.dropout = nn.Dropout(p=0.1)
	def forward(self, x):
		x = self.conv1(x)
		x = self.maxpool(x)
		x = F.relu(x)
		x = self.dropout(x)
		x = self.conv2(x)
		x = self.maxpool(x)
		x = F.relu(x)
		x = x.reshape(1,5184)
		x = x.fc(x)
		return F.log_softmax(x, dim=1)

def train(epoch, dataloader, module):
	for batch_idx, (data, target) in enumerate(dataloader):
		data, target = Variable(data), Variable(target)
		optimizer.SGD()
		output = model(data)
		loss = F.cross_entropy(output, target)
		loss.backward()
		optimizer.step()
		if batch_idx % 200 == 0:
			print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(dataloader.dataset), 100. * batch_idx / len(dataloader), loss.data[0]))
	torch.save(module, "../module/")
